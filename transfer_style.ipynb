{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils2\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from keras import metrics\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vgg16_avg import VGG16_Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils2.limit_mem() #comment out if not using shared GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load content image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/artisticTransf/images/*\"\n",
    "imgs = glob(path)\n",
    "len(imagenet_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(imagenet_imgs[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = img.size\n",
    "size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sty = Image.open('/home/ubuntu/artisticTransf/images/insertImage').resize(size, Image.ANTIALIAS) #need to match size with content image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for VGG and Imagenet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "preproc = lambda x: np.array(x - rgb_mean)[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deproc = lambda x, shape: np.clip(x.reshape(shape)[:, :, :, ::-1] + rgb_mean, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding dimensions to 4-dimensional tensors for keras\n",
    "img_arr = preproc(np.expand_dims(img, 0))\n",
    "sty_arr = preproc(np.expand_dims(sty, 0))\n",
    "shape = img_arr.shape; shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling variables in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_Avg(include_top=False, input_shape=shape[1:])\n",
    "name2output = {l.name: l.output for l in model.layers}\n",
    "outputs = [name2output['block{}_conv2'.format(o)] for o in range(1,6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model.input, outputs)\n",
    "cont_targs = [K.variable(o) for o in model.predict(img_arr)]\n",
    "sty_targs = [K.variable(o) for o in model.predict(sty_arr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss and Gradient Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_mat(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    return K.dot(features, K.transpose(features)) / x.get_shape().num_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sty_loss(targ, layer): return K.mean(metrics.mse(gram_mat(targ), gram_mat(layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_loss = sum(K.mean(metrics.mse(cont_targ[0], cont_outp[0])) \n",
    "                for cont_targ, cont_outp in zip(cont_targs, outputs))\n",
    "style_loss = sum(sty_loss(sty_targ[0], sty_outp[0]) \n",
    "                 for sty_targ, sty_outp in zip(sty_targs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.6*cont_loss + style_loss #reducing weight of content\n",
    "grad = K.gradients(loss, model.input)\n",
    "fn = K.function([model.input], [loss]+grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object): #need to separate the loss and gradients\n",
    "    def __init__(self, func, shape): self.func, self.shp = func, shape\n",
    "    \n",
    "    def loss(self, x):\n",
    "        loss_, self.grads_ = self.func([x.reshape(self.shp)])\n",
    "        return loss_.astype(np.float64)\n",
    "    \n",
    "    def grads(self, x): return self.grads_.flatten().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_obj = Evaluator(fn, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating initializing noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = lambda shape: np.random.uniform(0, 225, size=shape)\n",
    "x = noise(shape)\n",
    "plt.imshow(x[0].astype('uint8'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, shape, eval_obj, iters):\n",
    "    for i in range(iters):\n",
    "        x, min_val, info = fmin_l_bfgs_b(eval_obj.loss, x.flatten(), \n",
    "                                         fprime=eval_obj.grads, maxfun=20)\n",
    "        np.clip(x, -127, 127)\n",
    "        print('Current loss: {}'.format(min_val))\n",
    "        imsave(f'./results/res_at_iteration_{i}.png', deproc(x.copy(), shape)[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 20 #define your own iterators\n",
    "x = train(x, shape, eval_obj, iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Image.open('./results/res_at_iteration_19.png')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can add further display files here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
